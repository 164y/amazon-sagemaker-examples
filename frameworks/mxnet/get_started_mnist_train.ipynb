{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST training with MXNet and Gluon\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using MXNet and the Gluon API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "output_path='s3://' + sess.default_bucket() + '/mxnet/mnist'\n",
    "output_path = 's3://' + 'sagemaker-model-artifacts-dev' \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the entry point for training\n",
    "\n",
    "The entry point for training is a python script that provides all \n",
    "the code for training a MXNet model. It is used by the SageMaker \n",
    "MXNet Estimator (`MXNet` class above) as the entry point for running the training job.\n",
    "\n",
    "Under the hood, SageMaker MXNet Estimator downloads a docker image\n",
    "with runtime environemnts \n",
    "specified by the parameters you used to initiated the\n",
    "estimator class and it injects the training script into the \n",
    "docker image to be used as the entry point to run the container.\n",
    "\n",
    "In the rest of the notebook, we use *training image* to refer to the \n",
    "docker image specified by the MXNet Estimator and *training container*\n",
    "to refer to the container that runs the training image. \n",
    "\n",
    "This means your training script is very similar to a training script\n",
    "you might run outside Amazon SageMaker, but it can access the useful environment \n",
    "variables provided by the training image. Checkout [here](https://sagemaker.readthedocs.io/en/stable/frameworks/mxnet/using_mxnet.html?highlight=entry%20point) to see some common environment \n",
    "variables you might used. Checkout [here](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md) for a complete \n",
    "description of all environment variables your training script\n",
    "can access to. \n",
    "\n",
    "In this example, we will use the training script `code/train.py`\n",
    "as the entry point for our MXNet Estimator.\n",
    "\n",
    "The script here is an adaptation of the [Gluon MNIST example](https://github.com/apache/incubator-mxnet/blob/master/example/gluon/mnist.py) provided by the [Apache MXNet](https://mxnet.incubator.apache.org/) project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 'code/train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate the MXNet Estimator\n",
    "\n",
    "The MXNet estimator allows you to run your training script on SageMaker\n",
    "infrastracture in a containerized environment. You need to configure\n",
    "it with the following parameters to set up the environment:\n",
    "\n",
    "`role`: An IAM role to make AWS service requests.\n",
    "\n",
    "`instance_type`:\n",
    "The type of SageMaker instance to run your training script. \n",
    "Set it to `local` if you want to run the training job on \n",
    "the SageMaker instance you are using to run this notebook.\n",
    "\n",
    "`instance count`:\n",
    "The number of instances you need to run your training job. \n",
    "Multiple instances are needed for distributed training.\n",
    "\n",
    "`output_path`: \n",
    "S3 location to save training output (model artifacts and output files)\n",
    "\n",
    "`framework_version`: The version of MXNet you need to use.\n",
    "\n",
    "`py_version`: The python version you need to use.\n",
    "\n",
    "For more information, see [here](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase)\n",
    "\n",
    "### Set hyperparameters\n",
    "\n",
    "In addition, MXNet estimator allows you to parse command line arguments\n",
    "to your training script via `hyperparameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set local_mode to be True if you want to run the training script\n",
    "# on the machine that runs this notebook\n",
    "\n",
    "local_mode=False\n",
    "\n",
    "if local_mode:\n",
    "    instance_type='local'\n",
    "else:\n",
    "    instance_type='ml.c4.xlarge'\n",
    "    \n",
    "est = MXNet(\n",
    "    entry_point='train.py',\n",
    "    source_dir='code', # directory of your training script\n",
    "    role=role,\n",
    "    framework_version='1.7.0',\n",
    "    py_version='py3',\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        'batch-size':100,\n",
    "        'epochs':20,\n",
    "        'learning-rate': 0.1,\n",
    "        'momentum': 0.9,\n",
    "        'log-interval':100\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training container executes your training script like\n",
    "\n",
    "```\n",
    "python train.py --batch-size 100 --epochs 20 --learning-rate 0.1\n",
    "    --momentum 0.9 --log-interval 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up channels for training and testing data\n",
    "\n",
    "You need to tell `MXNet` estimator where to find your training and \n",
    "testing data. It can be a link to an S3 bucket or it can be a path\n",
    "in your local file system if you use local mode. In this example,\n",
    "we use a public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name=sess.boto_region_name\n",
    "\n",
    "train_data_loc='s3://sagemaker-sample-data-{}/mxnet/mnist/'.format(region_name)\n",
    "test_data_loc='s3://sagemaker-sample-data-{}/mxnet/mnist'.format(region_name)\n",
    "\n",
    "channels={\n",
    "    'training': train_data_loc,\n",
    "    'testing': test_data_loc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys of the dictionary `channels` are parsed to the training image\n",
    "and it creates the environment variable `SM_CHANNEL_<key name>`. \n",
    "\n",
    "In this example, `SM_CHANNEL_TRAINING` and `SM_CHANNEL_TESTING` are created in the training image (checkout \n",
    "how `code/train.py` access these variables). For more information,\n",
    "see: [SM_CHANNEL_{channel_name}](https://github.com/aws/sagemaker-training-toolkit/blob/master/ENVIRONMENT_VARIABLES.md#sm_channel_channel_name)\n",
    "\n",
    "If you want, you can create a channel for validation:\n",
    "```\n",
    "channels = {\n",
    "    'training': train_data_loc,\n",
    "    'validation': val_data_loc,\n",
    "    'test': test_data_loc\n",
    "    }\n",
    "```\n",
    "You can then access this channel within your training script via\n",
    "`SM_CHANNEL_VALIDATION`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training script on SageMaker\n",
    "Now, the training container has everything to execute your training\n",
    "script. You can start the container by calling `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-13 21:31:51 Starting - Starting the training job...\n",
      "2020-11-13 21:31:54 Starting - Launching requested ML instances......\n",
      "2020-11-13 21:33:04 Starting - Preparing the instances for training......\n",
      "2020-11-13 21:33:56 Downloading - Downloading input data...\n",
      "2020-11-13 21:34:45 Training - Training image download completed. Training in progress..\u001b[34m2020-11-13 21:34:46,567 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-11-13 21:34:46,569 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-13 21:34:46,582 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}', 'SM_USER_ENTRY_POINT': 'train.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"testing\",\"training\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'train', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-model-artifacts-dev/mxnet-training-2020-11-13-21-31-51-252/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-11-13-21-31-51-252\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-model-artifacts-dev/mxnet-training-2020-11-13-21-31-51-252/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}', 'SM_USER_ARGS': '[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TESTING': '/opt/ml/input/data/testing', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_BATCH-SIZE': '100', 'SM_HP_LOG-INTERVAL': '100', 'SM_HP_LEARNING-RATE': '0.1', 'SM_HP_EPOCHS': '20', 'SM_HP_MOMENTUM': '0.9'}\u001b[0m\n",
      "\u001b[34m2020-11-13 21:34:47,237 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-13 21:34:47,251 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-13 21:34:47,265 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-13 21:34:47,277 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 100,\n",
      "        \"log-interval\": 100,\n",
      "        \"learning-rate\": 0.1,\n",
      "        \"epochs\": 20,\n",
      "        \"momentum\": 0.9\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2020-11-13-21-31-51-252\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-model-artifacts-dev/mxnet-training-2020-11-13-21-31-51-252/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-model-artifacts-dev/mxnet-training-2020-11-13-21-31-51-252/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":100,\"epochs\":20,\"learning-rate\":0.1,\"log-interval\":100,\"momentum\":0.9},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-11-13-21-31-51-252\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-model-artifacts-dev/mxnet-training-2020-11-13-21-31-51-252/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"100\",\"--epochs\",\"20\",\"--learning-rate\",\"0.1\",\"--log-interval\",\"100\",\"--momentum\",\"0.9\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=100\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-INTERVAL=100\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=20\u001b[0m\n",
      "\u001b[34mSM_HP_MOMENTUM=0.9\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 train.py --batch-size 100 --epochs 20 --learning-rate 0.1 --log-interval 100 --momentum 0.9\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mDownloading /opt/ml/input/data/training/train/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443 \"GET /gluon/dataset/mnist/train-images-idx3-ubyte.gz HTTP/1.1\" 200 9912422\u001b[0m\n",
      "\u001b[34mDownloading /opt/ml/input/data/training/train/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443 \"GET /gluon/dataset/mnist/train-labels-idx1-ubyte.gz HTTP/1.1\" 200 28881\u001b[0m\n",
      "\u001b[34mDownloading /opt/ml/input/data/training/test/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443 \"GET /gluon/dataset/mnist/t10k-images-idx3-ubyte.gz HTTP/1.1\" 200 1648877\u001b[0m\n",
      "\u001b[34mDownloading /opt/ml/input/data/training/test/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443\u001b[0m\n",
      "\u001b[34mDEBUG:urllib3.connectionpool:https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com:443 \"GET /gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz HTTP/1.1\" 200 4542\u001b[0m\n",
      "\u001b[34m[2020-11-13 21:34:52.991 ip-10-0-177-80.us-west-2.compute.internal:17 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-11-13 21:34:52.991 ip-10-0-177-80.us-west-2.compute.internal:17 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-11-13 21:34:52.991 ip-10-0-177-80.us-west-2.compute.internal:17 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-11-13 21:34:52.991 ip-10-0-177-80.us-west-2.compute.internal:17 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-11-13 21:34:53.069 ip-10-0-177-80.us-west-2.compute.internal:17 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-11-13 21:34:53.069 ip-10-0-177-80.us-west-2.compute.internal:17 INFO hook.py:459] Hook is writing from the hook with pid: 17\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-11-13 21:34:53.082 ip-10-0-177-80.us-west-2.compute.internal:17 INFO hook.py:235] Registering hook for block softmaxcrossentropyloss0\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=2.319267511367798,Timestamp=1605303293.2044444,IterationNumber=0)\u001b[0m\n",
      "\u001b[34mDEBUG:root:metrics_file_path=/opt/ml/output/metrics/sagemaker/17.json\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 100] Training: accuracy=0.797624, 4033.799133 samples/s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[Epoch 0 Batch 200] Training: accuracy=0.860348, 2631.522019 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 300] Training: accuracy=0.884751, 2693.957981 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 400] Training: accuracy=0.900599, 3496.972678 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 500] Training: accuracy=0.910878, 3395.290327 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.08117630332708359,Timestamp=1605303307.5658958,IterationNumber=500)\u001b[0m\n",
      "\u001b[34m[Epoch 0] Training: accuracy=0.916983\u001b[0m\n",
      "\u001b[34m[Epoch 0] Validation: accuracy=0.957600\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 100] Training: accuracy=0.958812, 3947.766013 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 200] Training: accuracy=0.961592, 3393.971565 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 300] Training: accuracy=0.962226, 3429.240455 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1518828421831131,Timestamp=1605303320.9749928,IterationNumber=1000)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 400] Training: accuracy=0.962843, 3371.491500 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 500] Training: accuracy=0.963214, 3675.248635 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 1] Training: accuracy=0.964450\u001b[0m\n",
      "\u001b[34m[Epoch 1] Validation: accuracy=0.965700\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 100] Training: accuracy=0.977327, 3101.768190 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.04122145101428032,Timestamp=1605303334.7063153,IterationNumber=1500)\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 200] Training: accuracy=0.976119, 3440.972000 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 300] Training: accuracy=0.974419, 3301.821617 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 400] Training: accuracy=0.974040, 3666.445798 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 2 Batch 500] Training: accuracy=0.973653, 3194.078361 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 2] Training: accuracy=0.974183\u001b[0m\n",
      "\u001b[34m[Epoch 2] Validation: accuracy=0.971600\u001b[0m\n",
      "\u001b[34m[Epoch 3 Batch 100] Training: accuracy=0.980396, 3998.383222 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 3 Batch 200] Training: accuracy=0.978557, 3807.638328 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 3 Batch 300] Training: accuracy=0.978704, 4124.760537 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 3 Batch 400] Training: accuracy=0.978928, 3537.109125 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.11904434859752655,Timestamp=1605303361.934283,IterationNumber=2500)\u001b[0m\n",
      "\u001b[34m[Epoch 3 Batch 500] Training: accuracy=0.978762, 3671.806005 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 3] Training: accuracy=0.978767\u001b[0m\n",
      "\u001b[34m[Epoch 3] Validation: accuracy=0.974300\u001b[0m\n",
      "\u001b[34m[Epoch 4 Batch 100] Training: accuracy=0.983861, 3678.955863 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 4 Batch 200] Training: accuracy=0.983184, 3505.448346 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.117446668446064,Timestamp=1605303375.270817,IterationNumber=3000)\u001b[0m\n",
      "\u001b[34m[Epoch 4 Batch 300] Training: accuracy=0.982990, 3529.251792 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 4 Batch 400] Training: accuracy=0.982818, 3741.239854 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 4 Batch 500] Training: accuracy=0.982794, 3312.460710 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 4] Training: accuracy=0.982400\u001b[0m\n",
      "\u001b[34m[Epoch 4] Validation: accuracy=0.972700\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.048309050500392914,Timestamp=1605303389.1284087,IterationNumber=3500)\u001b[0m\n",
      "\u001b[34m[Epoch 5 Batch 100] Training: accuracy=0.985842, 3458.021964 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 5 Batch 200] Training: accuracy=0.986667, 3746.821150 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 5 Batch 300] Training: accuracy=0.986445, 3947.951807 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 5 Batch 400] Training: accuracy=0.985611, 3535.052129 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 5 Batch 500] Training: accuracy=0.985130, 3621.648879 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1244695857167244,Timestamp=1605303403.2137425,IterationNumber=4000)\u001b[0m\n",
      "\u001b[34m[Epoch 5] Training: accuracy=0.984783\u001b[0m\n",
      "\u001b[34m[Epoch 5] Validation: accuracy=0.964100\u001b[0m\n",
      "\u001b[34m[Epoch 6 Batch 100] Training: accuracy=0.985446, 3750.473023 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 6 Batch 200] Training: accuracy=0.986418, 4129.796577 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 6 Batch 300] Training: accuracy=0.987475, 3696.920338 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.01654095947742462,Timestamp=1605303416.5555437,IterationNumber=4500)\u001b[0m\n",
      "\u001b[34m[Epoch 6 Batch 400] Training: accuracy=0.987107, 3968.458998 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 6 Batch 500] Training: accuracy=0.987166, 3502.579562 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 6] Training: accuracy=0.986983\u001b[0m\n",
      "\u001b[34m[Epoch 6] Validation: accuracy=0.968300\u001b[0m\n",
      "\u001b[34m[Epoch 7 Batch 100] Training: accuracy=0.990000, 3034.513095 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.011390422470867634,Timestamp=1605303430.211456,IterationNumber=5000)\u001b[0m\n",
      "\u001b[34m[Epoch 7 Batch 200] Training: accuracy=0.990348, 3343.566851 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 7 Batch 300] Training: accuracy=0.989635, 3338.085157 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 7 Batch 400] Training: accuracy=0.988903, 3577.597707 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 7 Batch 500] Training: accuracy=0.988603, 2911.983115 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 7] Training: accuracy=0.988417\u001b[0m\n",
      "\u001b[34m[Epoch 7] Validation: accuracy=0.974600\u001b[0m\n",
      "\u001b[34m[Epoch 8 Batch 100] Training: accuracy=0.990594, 4300.042034 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 8 Batch 200] Training: accuracy=0.991244, 3597.543486 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 8 Batch 300] Training: accuracy=0.991495, 3337.925766 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 8 Batch 400] Training: accuracy=0.991621, 3048.429744 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.01808527670800686,Timestamp=1605303457.710646,IterationNumber=6000)\u001b[0m\n",
      "\u001b[34m[Epoch 8 Batch 500] Training: accuracy=0.991936, 3523.440860 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 8] Training: accuracy=0.991333\u001b[0m\n",
      "\u001b[34m[Epoch 8] Validation: accuracy=0.964400\u001b[0m\n",
      "\u001b[34m[Epoch 9 Batch 100] Training: accuracy=0.990693, 3821.515193 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 9 Batch 200] Training: accuracy=0.991343, 3505.213983 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.008336528204381466,Timestamp=1605303471.4920552,IterationNumber=6500)\u001b[0m\n",
      "\u001b[34m[Epoch 9 Batch 300] Training: accuracy=0.991595, 3923.246874 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 9 Batch 400] Training: accuracy=0.991247, 3747.189365 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 9 Batch 500] Training: accuracy=0.991058, 3539.347707 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 9] Training: accuracy=0.990633\u001b[0m\n",
      "\u001b[34m[Epoch 9] Validation: accuracy=0.976000\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0018488592468202114,Timestamp=1605303485.115718,IterationNumber=7000)\u001b[0m\n",
      "\u001b[34m[Epoch 10 Batch 100] Training: accuracy=0.993762, 3538.780331 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 10 Batch 200] Training: accuracy=0.993831, 3671.548872 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 10 Batch 300] Training: accuracy=0.992724, 3978.245488 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 10 Batch 400] Training: accuracy=0.992145, 3479.710626 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 10 Batch 500] Training: accuracy=0.992216, 3711.412163 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.012142548337578773,Timestamp=1605303499.2047613,IterationNumber=7500)\u001b[0m\n",
      "\u001b[34m[Epoch 10] Training: accuracy=0.992200\u001b[0m\n",
      "\u001b[34m[Epoch 10] Validation: accuracy=0.975200\u001b[0m\n",
      "\u001b[34m[Epoch 11 Batch 100] Training: accuracy=0.993267, 3426.774947 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 11 Batch 200] Training: accuracy=0.993831, 3895.951996 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 11 Batch 300] Training: accuracy=0.993289, 2946.100247 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.006539647001773119,Timestamp=1605303512.7327032,IterationNumber=8000)\u001b[0m\n",
      "\u001b[34m[Epoch 11 Batch 400] Training: accuracy=0.992918, 3699.463731 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 11 Batch 500] Training: accuracy=0.992435, 3705.346479 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 11] Training: accuracy=0.991950\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[Epoch 11] Validation: accuracy=0.972600\u001b[0m\n",
      "\u001b[34m[Epoch 12 Batch 100] Training: accuracy=0.991980, 3801.633297 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.00780603289604187,Timestamp=1605303526.5178218,IterationNumber=8500)\u001b[0m\n",
      "\u001b[34m[Epoch 12 Batch 200] Training: accuracy=0.992786, 3493.070164 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 12 Batch 300] Training: accuracy=0.992591, 3519.213311 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 12 Batch 400] Training: accuracy=0.992170, 3770.703202 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 12 Batch 500] Training: accuracy=0.991697, 3354.584426 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 12] Training: accuracy=0.991850\u001b[0m\n",
      "\u001b[34m[Epoch 12] Validation: accuracy=0.976500\u001b[0m\n",
      "\u001b[34m[Epoch 13 Batch 100] Training: accuracy=0.995248, 3948.026130 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 13 Batch 200] Training: accuracy=0.994080, 3481.992744 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 13 Batch 300] Training: accuracy=0.993023, 3516.705235 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 13 Batch 400] Training: accuracy=0.993242, 3164.915299 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.005130572244524956,Timestamp=1605303553.9914465,IterationNumber=9500)\u001b[0m\n",
      "\u001b[34m[Epoch 13 Batch 500] Training: accuracy=0.992874, 3401.320207 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 13] Training: accuracy=0.992450\u001b[0m\n",
      "\u001b[34m[Epoch 13] Validation: accuracy=0.975800\u001b[0m\n",
      "\u001b[34m[Epoch 14 Batch 100] Training: accuracy=0.994356, 3688.272951 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 14 Batch 200] Training: accuracy=0.995075, 3214.641885 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0016310320934280753,Timestamp=1605303567.6677067,IterationNumber=10000)\u001b[0m\n",
      "\u001b[34m[Epoch 14 Batch 300] Training: accuracy=0.993787, 3507.001789 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 14 Batch 400] Training: accuracy=0.993292, 3510.994291 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 14 Batch 500] Training: accuracy=0.992934, 3607.910333 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 14] Training: accuracy=0.992700\u001b[0m\n",
      "\u001b[34m[Epoch 14] Validation: accuracy=0.976800\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.021331029012799263,Timestamp=1605303581.5421343,IterationNumber=10500)\u001b[0m\n",
      "\u001b[34m[Epoch 15 Batch 100] Training: accuracy=0.994554, 3307.967254 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 15 Batch 200] Training: accuracy=0.995124, 3608.406962 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 15 Batch 300] Training: accuracy=0.994352, 3807.914877 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 15 Batch 400] Training: accuracy=0.993965, 4266.449664 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 15 Batch 500] Training: accuracy=0.994232, 3651.189554 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.019043857231736183,Timestamp=1605303595.3022268,IterationNumber=11000)\u001b[0m\n",
      "\u001b[34m[Epoch 15] Training: accuracy=0.994433\u001b[0m\n",
      "\u001b[34m[Epoch 15] Validation: accuracy=0.976200\u001b[0m\n",
      "\u001b[34m[Epoch 16 Batch 100] Training: accuracy=0.995545, 3453.182065 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 16 Batch 200] Training: accuracy=0.995522, 3650.204515 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 16 Batch 300] Training: accuracy=0.994551, 3363.596558 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.012197443284094334,Timestamp=1605303608.778537,IterationNumber=11500)\u001b[0m\n",
      "\u001b[34m[Epoch 16 Batch 400] Training: accuracy=0.994040, 3732.218080 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 16 Batch 500] Training: accuracy=0.993393, 3211.024177 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 16] Training: accuracy=0.993267\u001b[0m\n",
      "\u001b[34m[Epoch 16] Validation: accuracy=0.978100\u001b[0m\n",
      "\u001b[34m[Epoch 17 Batch 100] Training: accuracy=0.993663, 3244.532114 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.03405913710594177,Timestamp=1605303621.9679477,IterationNumber=12000)\u001b[0m\n",
      "\u001b[34m[Epoch 17 Batch 200] Training: accuracy=0.993234, 2381.260148 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 17 Batch 300] Training: accuracy=0.992857, 3791.358426 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 17 Batch 400] Training: accuracy=0.992818, 3490.657301 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 17 Batch 500] Training: accuracy=0.992535, 3373.932349 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 17] Training: accuracy=0.992767\u001b[0m\n",
      "\u001b[34m[Epoch 17] Validation: accuracy=0.974900\u001b[0m\n",
      "\u001b[34m[Epoch 18 Batch 100] Training: accuracy=0.995050, 3382.639623 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 18 Batch 200] Training: accuracy=0.995075, 3950.926903 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 18 Batch 300] Training: accuracy=0.994850, 3464.505844 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 18 Batch 400] Training: accuracy=0.995012, 3312.068353 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0001649707992328331,Timestamp=1605303649.3788688,IterationNumber=13000)\u001b[0m\n",
      "\u001b[34m[Epoch 18 Batch 500] Training: accuracy=0.995050, 3578.482881 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 18] Training: accuracy=0.994933\u001b[0m\n",
      "\u001b[34m[Epoch 18] Validation: accuracy=0.977200\u001b[0m\n",
      "\u001b[34m[Epoch 19 Batch 100] Training: accuracy=0.994059, 3731.388004 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 19 Batch 200] Training: accuracy=0.994428, 3638.141334 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.0016175657510757446,Timestamp=1605303662.8858101,IterationNumber=13500)\u001b[0m\n",
      "\u001b[34m[Epoch 19 Batch 300] Training: accuracy=0.993787, 3595.754677 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 19 Batch 400] Training: accuracy=0.993017, 3806.014410 samples/s\u001b[0m\n",
      "\n",
      "2020-11-13 21:41:20 Uploading - Uploading generated training model\u001b[34m[Epoch 19 Batch 500] Training: accuracy=0.993573, 3470.296120 samples/s\u001b[0m\n",
      "\u001b[34m[Epoch 19] Training: accuracy=0.993767\u001b[0m\n",
      "\u001b[34m[Epoch 19] Validation: accuracy=0.975300\u001b[0m\n",
      "\u001b[34m2020-11-13 21:41:16,874 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-11-13 21:41:27 Completed - Training job completed\n",
      "Training seconds: 451\n",
      "Billable seconds: 451\n"
     ]
    }
   ],
   "source": [
    "est.fit(inputs=channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and store model data\n",
    "\n",
    "Now, the training is finished, the model artifact has been saved in \n",
    "the `output_path`. We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact saved at:\n",
      " s3://sagemaker-model-artifacts-dev/mxnet-training-2020-11-13-21-31-51-252/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = est.model_data\n",
    "print(\"Model artifact saved at:\\n\", model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store the variable `model_data` in the current notebook kernel. \n",
    "In the [next notebook](get_started_with_mnist_deploy.ipynb), you will learn how to retrieve the model artifact and deploy to a SageMaker\n",
    "Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'model_data' (str)\n"
     ]
    }
   ],
   "source": [
    "%store model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

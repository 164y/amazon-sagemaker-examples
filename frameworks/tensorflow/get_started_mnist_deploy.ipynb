{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy a Trained Model\n",
    "\n",
    "In this notebook, we walk through the process of deploying a trained model to a SageMaker endpoint. If you recently ran [the notebook for training](get_started_mnist_deploy.ipynb) with %store% magic, the `model_data` can be restored. Otherwise, we retrieve the \n",
    "model artifact from a public S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get global config\n",
    "with open('code/config.json', 'r') as f:\n",
    "    CONFIG=json.load(f)\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "%store -r tf_mnist_model_data\n",
    "\n",
    "try: \n",
    "    tf_mnist_model_data\n",
    "except NameError:\n",
    "    import json\n",
    "    tf_mnist_model_data = 's3://' + CONFIG['public_bucket'] + '/datasets/image/MNIST/model/tensorflow-training-2020-11-20-23-57-13-077/model.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-west-2-688520471316/tensorflow/mnist/tensorflow-training-2020-11-24-02-57-47-196/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(tf_mnist_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Model Object\n",
    "\n",
    "The `TensorFlowModel` class allows you to define an environment for making inference using your\n",
    "model artifact. Like `TensorFlow` estimator class we discussed \n",
    "[in this notebook for training an Tensorflow model](\n",
    "get_started_mnist_train.ipynb), it is high level API used to set up a docker image for your model hosting service.\n",
    "\n",
    "Once it is properly configured, it can be used to create a SageMaker\n",
    "endpoint on an EC2 instance. The SageMaker endpoint is a containerized environment that uses your trained model \n",
    "to make inference on incoming data via RESTful API calls. \n",
    "\n",
    "Some common parameters used to initiate the `TensorFlowModel` class are:\n",
    "- role: An IAM role to make AWS service requests\n",
    "- model_data: the S3 bucket URI of the compressed model artifact. It can be a path to a local file if the endpoint \n",
    "is to be deployed on the SageMaker instance you are using to run this notebook (local mode)\n",
    "- framework_version: version of the MXNet package to be used\n",
    "- py_version: python version to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TensorFlowModel(\n",
    "    role=role,\n",
    "    model_data=tf_mnist_model_data,\n",
    "    framework_version='2.3.0',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Inference Container\n",
    "Once the `TensorFlowModel` class is initiated, we can call its `deploy` method to run the container for the hosting\n",
    "service. Some common parameters needed to call `deploy` methods are:\n",
    "\n",
    "- initial_instance_count: the number of SageMaker instances to be used to run the hosting service.\n",
    "- instance_type: the type of SageMaker instance to run the hosting service. Set it to `local` if you want run the hosting service on the local SageMaker instance. Local mode are typically used for debugging. \n",
    "\n",
    "<span style=\"color:red\"> Note: local mode is not supported in SageMaker Studio </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Using the short-lived AWS credentials found in session. They might expire while running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmp1w4bp5gk_algo-1-u1a9n_1\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:starting services\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:tfs_utils:using default model name: model\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:tfs_utils:tensorflow serving model config: \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m model_config_list: {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   config: {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     name: \"model\",\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     base_path: \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:using default model name: model\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:tensorflow serving model config: \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m model_config_list: {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   config: {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     name: \"model\",\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     base_path: \"/opt/ml/model\",\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     model_platform: \"tensorflow\"\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:tensorflow version info:\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m TensorFlow ModelServer: 2.3.0-rc0+dev.sha.no_git\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m TensorFlow Library: 2.3.0\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=/sagemaker/model-config.cfg --max_num_load_retries=0 \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:started tensorflow serving (pid: 11)\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:nginx config: \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m load_module modules/ngx_http_js_module.so;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m worker_processes auto;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m daemon off;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m pid /tmp/nginx.pid;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m error_log  /dev/stderr error;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m worker_rlimit_nofile 4096;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m events {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   worker_connections 2048;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m http {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   include /etc/nginx/mime.types;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   default_type application/json;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   access_log /dev/stdout combined;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   js_include tensorflow-serving.js;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   upstream tfs_upstream {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     server localhost:8501;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   upstream gunicorn_upstream {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     server unix:/tmp/gunicorn.sock fail_timeout=1;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   server {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     listen 8080 deferred;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     client_max_body_size 0;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     client_body_buffer_size 100m;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     subrequest_output_buffer_size 100m;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     set $tfs_version 2.3;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     set $default_tfs_model model;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     location /tfs {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         rewrite ^/tfs/(.*) /$1  break;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         proxy_redirect off;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         proxy_pass_request_headers off;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         proxy_set_header Content-Type 'application/json';\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         proxy_set_header Accept 'application/json';\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         proxy_pass http://tfs_upstream;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     location /ping {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         js_content ping;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     location /invocations {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         js_content invocations;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     location /models {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         proxy_pass http://gunicorn_upstream/models;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     location / {\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m         return 404 '{\"error\": \"Not Found\"}';\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m     keepalive_timeout 3;\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m   }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:nginx version info:\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m nginx version: nginx/1.18.0\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m built by gcc 7.4.0 (Ubuntu 7.4.0-1ubuntu1~18.04.1) \n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m built with OpenSSL 1.1.1  11 Sep 2018 (running with OpenSSL 1.1.1g  21 Apr 2020)\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m TLS SNI support enabled\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fdebug-prefix-map=/data/builder/debuild/nginx-1.18.0/debian/debuild-base/nginx-1.18.0=. -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m INFO:__main__:started nginx (pid: 13)\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.199562: I tensorflow_serving/model_servers/server_core.cc:464] Adding/updating models.\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.199596: I tensorflow_serving/model_servers/server_core.cc:575]  (Re-)adding model: model\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.299859: I tensorflow_serving/util/retrier.cc:46] Retrying of Reserving resources for servable: {name: model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.299893: I tensorflow_serving/core/basic_manager.cc:739] Successfully reserved resources to load servable {name: model version: 0}\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.299903: I tensorflow_serving/core/loader_harness.cc:66] Approving load for servable version {name: model version: 0}\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.299912: I tensorflow_serving/core/loader_harness.cc:74] Loading servable version {name: model version: 0}\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.299973: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:31] Reading SavedModel from: /opt/ml/model/00000000\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.302370: I external/org_tensorflow/tensorflow/cc/saved_model/reader.cc:54] Reading meta graph with tags { serve }\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.302395: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:234] Reading SavedModel debug info (if present) from: /opt/ml/model/00000000\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.303409: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.306360: I external/org_tensorflow/tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.338653: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:199] Restoring SavedModel bundle.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.403090: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: /opt/ml/model/00000000\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.406491: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:303] SavedModel load for tags { serve }; Status: success: OK. Took 106547 microseconds.\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.406864: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /opt/ml/model/00000000/assets.extra/tf_serving_warmup_requests\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.407129: I tensorflow_serving/util/retrier.cc:46] Retrying of Loading servable: {name: model version: 0} exhausted max_num_retries: 0\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.407143: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model version: 0}\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.416220: I tensorflow_serving/model_servers/server.cc:367] Running gRPC ModelServer at 0.0.0.0:9000 ...\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m [warn] getaddrinfo: address family for nodename not supported\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m 2020-11-24 03:01:40.419863: I tensorflow_serving/model_servers/server.cc:387] Exporting HTTP/REST API at:localhost:8501 ...\n",
      "\u001b[36malgo-1-u1a9n_1  |\u001b[0m [evhttp_server.cc : 238] NET_LOG: Entering the event loop ...\n",
      "!\u001b[36malgo-1-u1a9n_1  |\u001b[0m 172.18.0.1 - - [24/Nov/2020:03:01:43 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"-\"\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# set local_mode to False if you want to deploy on a remote\n",
    "# SageMaker instance\n",
    "\n",
    "local_mode=True\n",
    "\n",
    "if local_mode:\n",
    "    instance_type='local'\n",
    "else:\n",
    "    instance_type='ml.c4.xlarge'\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions Against a SageMaker endpoint\n",
    "\n",
    "Once you have the `Predictor` instance returned by `model.deploy(...)`, you can send prediction requests to your endpoints. In this case, the model accepts normalized \n",
    "batch images in depth-minor convention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some dummy inputs\n",
    "import numpy as np\n",
    "\n",
    "dummy_inputs = {\n",
    "    'instances': np.random.rand(4, 28, 28, 1)\n",
    "}\n",
    "\n",
    "res = predictor.predict(dummy_inputs)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formats of the input and output data correspond directly to the request and response\n",
    "format of the `Predict` method in [TensorFlow Serving REST API](https://www.tensorflow.org/tfx/serving/api_rest), for example, the key of the array to be \n",
    "parsed to the model in the `dummy_inputs` needs to be called `instances`. Moreover, the input data needs to have a batch dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to see an example that cannot be processed by the endpoint\n",
    "\n",
    "#dummy_data = {\n",
    "#    'instances': np.random.rand(28, 28, 1).tolist()\n",
    "#}\n",
    "#print(predictor.predict(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use real MNIST test to test the endpoint. We use helper functions defined in `code.utils` to \n",
    "download MNIST data set and normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mnist import mnist_to_numpy, normalize\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data_dir = '/tmp/data'\n",
    "X, _ = mnist_to_numpy(data_dir, train=False)\n",
    "\n",
    "# randomly sample 16 images to inspect\n",
    "mask = random.sample(range(X.shape[0]), 16)\n",
    "samples = X[mask]\n",
    "\n",
    "# plot the images \n",
    "fig, axs = plt.subplots(nrows=1, ncols=16, figsize=(16, 1))\n",
    "\n",
    "for i, splt in enumerate(axs):\n",
    "    splt.imshow(samples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model accepts normalized input, you will need to normalize the samples before \n",
    "sending it to the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = normalize(samples, axis=(1, 2))\n",
    "predictions = predictor.predict(\n",
    "    np.expand_dims(samples, 3) # add channel dim\n",
    ")['predictions'] \n",
    "\n",
    "# softmax to logit\n",
    "predictions = np.array(predictions, dtype=np.float32)\n",
    "predictions = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions: \", predictions.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Clean up \n",
    "\n",
    "If you do not plan to use the endpoint, you should delete it to free up some computation \n",
    "resource. If you use local, you will need to manually delete the docker container bounded\n",
    "at port 8080 (the port that listens to the incoming request).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not local_mode:\n",
    "    predictor.delete_endpoint()\n",
    "else:\n",
    "    os.system(\"docker container ls | grep 8080 | awk '{print $1}' | xargs docker container rm -f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
